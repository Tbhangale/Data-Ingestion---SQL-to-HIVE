Step-by-step approach for daily data ingestion from SQL to HIVE:

1. Create respective table named directories in hdfs. In this project, I have followed retail_db dataset with tables named orders, order_items, products, categories, customers, departments

$ hadoop fs -mkdir /user/tbhangale/sqoop_import/retail_db/orders
$ hadoop fs -mkdir /user/tbhangale/sqoop_import/retail_db/order_items
$ hadoop fs -mkdir /user/tbhangale/sqoop_import/retail_db/products
$ hadoop fs -mkdir /user/tbhangale/sqoop_import/retail_db/categories
$ hadoop fs -mkdir /user/tbhangale/sqoop_import/retail_db/customers
$ hadoop fs -mkdir /user/tbhangale/sqoop_import/retail_db/departments

2. Using sqoop, import data from SQL tables to a newly created directory marked by date(yesterday's date logically). 
This directory will hold the data for a respective partition of hive table(to be created). 

For Transactional tables:
-------------------------
$ sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--fields-terminated-by ',' \
--target-dir /user/tbhangale/sqoop_import/retail_db/orders/order_date=2013-07-25 \
-e "select order_id, order_customer_id, order_status, order_date from orders where DATE(order_date)='2013-07-25' AND \$CONDITIONS" \
--split-by order_id;

$ sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \	
--username retail_user \
--password itversity \
--fields-terminated-by ',' \
--target-dir /user/tbhangale/sqoop_import/retail_db/order_items/load_date=2013-07-25 \
-e "select oi.order_item_id,oi.order_item_order_id,oi.order_item_product_id,oi.order_item_quantity,oi.order_item_subtotal,
oi.order_item_product_price, o.order_date as load_date from order_items oi inner join orders o on o.order_id = oi.order_item_order_id where DATE(o.order_date)='2013-07-25' AND \$CONDITIONS" \
--split-by oi.order_item_id;

For Dimensional tables:
-----------------------
$ sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--fields-terminated-by ',' \
--target-dir /user/tbhangale/sqoop_import/retail_db/customers/load_date=2013-07-25 \
-e "select customer_id, customer_fname,customer_lname,customer_email,customer_password,customer_street,customer_city,customer_state,
customer_zipcode, '2013-07-25' as load_date from customers where \$CONDITIONS" \
--split-by customer_id;

$ sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--fields-terminated-by ',' \
--target-dir /user/tbhangale/sqoop_import/retail_db/categories/load_date=2013-07-25 \
-e "select category_id,category_department_id,category_name,'2013-07-25' as load_date from categories where \$CONDITIONS" \
--split-by category_id;

$ sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--fields-terminated-by ',' \
--target-dir /user/tbhangale/sqoop_import/retail_db/departments/load_date=2013-07-25 \
-e "select department_id,department_name,'2013-07-25' as load_date from departments where \$CONDITIONS" \
--split-by department_id;

$ sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--fields-terminated-by ',' \
--target-dir /user/tbhangale/sqoop_import/retail_db/products/load_date=2013-07-25 \
-e "select product_id,product_category_id,product_name,product_description,product_price,product_image,'2013-07-25' as load_date from products where \$CONDITIONS" \
--split-by product_id;

3. Check if the directories along with partitioned folders are created in HDFS or not:
$ hadoop fs -ls /user/tbhangale/sqoop_import/retail_db/*/*/

4. Create hive external tables partitioned by date
hive>
CREATE EXTERNAL TABLE IF NOT EXISTS 
tbhangale.orders(order_id INT, order_customer_id INT,order_status STRING)
PARTITIONED BY (order_date DATE) 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',' 
STORED AS TEXTFILE 
LOCATION '/user/tbhangale/sqoop_import/retail_db/orders';

CREATE EXTERNAL TABLE IF NOT EXISTS 
tbhangale.order_items(order_item_id INT,order_item_order_id INT,order_item_product_id INT,order_item_quantity INT,order_item_subtotal FLOAT,order_item_product_price FLOAT)
PARTITIONED BY (load_date DATE)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',' 
STORED AS TEXTFILE 
LOCATION '/user/tbhangale/sqoop_import/retail_db/order_items';

CREATE EXTERNAL TABLE IF NOT EXISTS 
tbhangale.customers(customer_id INT, customer_fname STRING,customer_lname STRING,customer_email STRING,customer_password STRING,customer_street STRING,customer_city STRING,customer_state STRING,customer_zipcode INT)
PARTITIONED BY (load_date DATE) 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',' 
STORED AS TEXTFILE 
LOCATION '/user/tbhangale/sqoop_import/retail_db/customers';

CREATE EXTERNAL TABLE IF NOT EXISTS 
tbhangale.categories(category_id INT,category_department_id INT,category_name STRING)
PARTITIONED BY (load_date DATE) 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',' 
STORED AS TEXTFILE 
LOCATION '/user/tbhangale/sqoop_import/retail_db/categories';

CREATE EXTERNAL TABLE IF NOT EXISTS 
tbhangale.departments(department_id INT,department_name STRING)
PARTITIONED BY (load_date DATE) 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',' 
STORED AS TEXTFILE 
LOCATION '/user/tbhangale/sqoop_import/retail_db/departments';

CREATE EXTERNAL TABLE IF NOT EXISTS 
tbhangale.products(product_id INT,product_category_id INT,product_name STRING,product_description STRING,product_price FLOAT,product_image STRING)
PARTITIONED BY (load_date DATE) 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',' 
STORED AS TEXTFILE 
LOCATION '/user/tbhangale/sqoop_import/retail_db/products';

5. Update hive metastore with above imported hdfs data 
hive>
MSCK REPAIR TABLE tbhangale.orders;
MSCK REPAIR TABLE tbhangale.order_items;
MSCK REPAIR TABLE tbhangale.customers;
MSCK REPAIR TABLE tbhangale.categories;
MSCK REPAIR TABLE tbhangale.departments;
MSCK REPAIR TABLE tbhangale.products;

Note: Step-2 and setp-5 can be automated in a script to be run daily, so it will update hive tables and hdfs directories with yesterday's data from SQL tables. I have created orders.sh for the same for orders table. 
